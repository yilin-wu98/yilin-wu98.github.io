---
---

@string{aps = {American Physical Society,}}
@inproceedings{tian2024pre,
title = {Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment},
author = {Tian, Ran and Wu, Yilin and Xu, Chenfeng and Tomizuka, Masayoshi and Malik, Jitendra and Bajcsy, Andrea},
booktitle = {submission to  the International Journal of Robotics Researchâ€™s (IJRR) special issue on Foundation Models and Neural-Symbolic AI for Robotics},
year = {2024},
abbr = {arxiv},
selected = {true},
bibtex_show = {true},
preview = {rapl.gif},
website = {https://sites.google.com/berkeley.edu/rapl},
arxiv = {2412.04835},
abstract = {Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains a challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user's visual representation and then constructs a dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment.},
}
  
@inproceedings{qitooluse2024,
  title={Learning Generalizable Tool-use Skills through Trajectory Generation},    
  author={Qi*, Carl and Wu*, Yilin and Yu, Lifan and Liu, Haoyue and Jiang, Bowen and Lin**, Xingyu and Held**, David},   
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},   
  year={2024}  ,
  abbr = {IROS},
  selected = {true},
  bibtex_show = {true},
  preview = {toolgen.gif},
  website = {https://sites.google.com/view/toolgen},
  video = {https://youtu.be/iheVOX8jG_8},
  arxiv = {2310.00156},
  abstract = {Autonomous systems that efficiently utilize tools can assist humans in completing many common tasks such as cooking and cleaning. However, current systems fall short of matching human-level of intelligence in terms of adapting to novel tools. Prior works based on affordance often make strong assumptions about the environments and cannot scale to more complex, contact-rich tasks. In this work, we tackle this challenge and explore how agents can learn to use previously unseen tools to manipulate deformable objects. We propose to learn a generative model of the tool-use trajectories as a sequence of tool point clouds, which generalizes to different tool shapes. Given any novel tool, we first generate a tool-use trajectory and then optimize the sequence of tool poses to align with the generated trajectory. We train a single model on four different challenging deformable object manipulation tasks, using demonstration data from only one tool per task. The model generalizes to various novel tools, significantly outperforming baselines. We further test our trained policy in the real world with unseen tools, where it achieves the performance comparable to human.},

}



@inproceedings{jiang2024hacman++,
  title={HACMan++: Spatially-Grounded Motion Primitives for Manipulation},
  author={Jiang*, Bowen and Wu*, Yilin and Zhou, Wenxuan and Paxton, Chris and Held, David},
  booktitle={Robotics: Science and Systems (RSS)},
  year={2024},
  arxiv = {2407.08585},
  website = {https://sgmp-rss2024.github.io/},
  preview = {hacman.gif},
  abbr = {RSS},
  selected = {true},
  video = {https://sgmp-rss2024.github.io/static/videos/all_v2.mp4},
  bibtex_show = {true},
  abstract = {In this work, we introduce spatially-grounded parameterized motion primitives to improve policy generalization for robotic manipulation tasks. By grounding the primitives on a spatial location in the environment, our proposed method is able to effectively generalize across object shape and pose variations.}
}



@inproceedings{khazatsky2024droid,
    title   = {DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset},
    author = {Team, DROID Dataset},
    booktitle={Robotics: Science and Systems (RSS)},
    year = {2024},
    abbr = {RSS},
    selected = {true},
    bibtex_show = {true},
    preview = {droid.gif},
    website = {https://droid-dataset.github.io/},
    video = {https://droid-dataset.github.io/videos/droid_teaser_animated.mp4},
    arxiv = {2403.12945},
    abstract = {In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350h of interaction data, collected across 564 scenes and 86 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance, greater robustness, and improved generalization ability.},
}

@inproceedings{open_x_embodiment_rt_x_2023,
title={Open {X-E}mbodiment: Robotic Learning Datasets and {RT-X} Models},
author = {Open X-Embodiment Team},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
award = {Best Paper Award},
abbr = {ICRA},
year = {2024},
bibtex_show = {true},
selected = {true},
preview = {openx.gif},
website = {https://robotics-transformer-x.github.io/},
video = {https://robotics-transformer-x.github.io/video/teaser_compressed.mp4},
arxiv = {2310.08864},
}
@inproceedings{grannen2023stabilize,
	title={Stabilize to Act: Learning to Coordinate for Bimanual Manipulation},
	author={Grannen, Jennifer and Wu, Yilin and Vu, Brandon and Sadigh, Dorsa},
	booktitle={Proceedings of the 7th Conference on Robotic Learning (CoRL)},
	year={2023},
  abbr = {CoRL},
  selected = {true},
  bibtex_show = {true},
  preview = {buds.gif},
  website = {https://sites.google.com/view/stabilizetoact},
  video = {https://www.youtube.com/watch?v=nog-fOgFKdc},
  arxiv = {2309.01087},
  abstract = {We present a system for bimanual manipulation that coordinates by assigning roles to arms: a stabilizing arm holds an object stationary while an acting arm acts in this simplified environment.},
  award = {Oral Presentation [6.6%]},
}

@inproceedings{10160467,
  author={Shaikewitz*, Lorenzo and Wu*, Yilin and Belkhale*, Suneel and Grannen, Jennifer and Sundaresan, Priya and Sadigh, Dorsa},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={In-Mouth Robotic Bite Transfer with Visual and Haptic Sensing}, 
  year={2023},
  abbr = {ICRA},
  selected = {true},
  bibtex_show = {true},
  preview = {bite_transfer.gif},
  website = {https://sites.google.com/view/bitetransfericra2023/home},
  video = {https://www.youtube.com/watch?v=2FeDwEpunt0&ab_channel=LorenzoShaikewitz},
  arxiv = {2211.12705},
  abstract = {We build a semi autonomous robotic system to do in-mouth transfer of food safely and comfortably for disabled people. The system is composed of a force-reactive controller to safely accommodate the motions of the user throughout the transfer, a novel dexterous wrist-like end effector to reduce the discomfort and a visual sensor to identify the user mouth.},
  blog = {https://hai.stanford.edu/news/building-precise-assistive-feeding-robot-can-handle-any-meal?utm_source=Stanford+HAI&utm_campaign=d8369bf02f-Mailchimp_HAI_Newsletter_April+2023_3_Stanford&utm_medium=email&utm_term=0_aaf04f4a4b-d8369bf02f-214062174},

}

@inproceedings{grannen2022learning,
  title={Learning Bimanual Scooping Policies for Food Acquisition},
   author={Grannen*, Jennifer and Wu*, Yilin and Belkhale, Suneel and Sadigh, Dorsa},
  booktitle={Proceedings of the 6th Conference on Robot Learning (CoRL)},
  year={2022},
  abbr = {CoRL},
  selected = {true},
  bibtex_show = {true},
  preview = {carbs_after.gif},
  website = {https://sites.google.com/view/bimanualscoop-corl22/home},
  video = {https://drive.google.com/file/d/1kH6bAgOLegxePHUbKUTDc5SfQpftNgsn/view?usp=sharing},
  arxiv = {2211.14652},
  abstract = {We propose a general bimanual scooping primitive and an adaptive stabilization strategy that enables successful acquisition of a diverse set of food geometries and physical properties with close-loop visual feedback.},
  blog = {https://hai.stanford.edu/news/building-precise-assistive-feeding-robot-can-handle-any-meal?utm_source=Stanford+HAI&utm_campaign=d8369bf02f-Mailchimp_HAI_Newsletter_April+2023_3_Stanford&utm_medium=email&utm_term=0_aaf04f4a4b-d8369bf02f-214062174},
  }


@inproceedings{li2021solving,
    title={Solving Compositional Reinforcement Learning Problems via Task Reduction},
    author={Li, Yunfei and Wu, Yilin and Xu, Huazhe and Wang, Xiaolong and Wu, Yi},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2021},
    selected = {true},  
    bibtex_show = {true},
    abbr = {ICLR},  
    preview = {sir_fetch.gif},
    video = {https://slideslive.com/38941385/solving-compositional-reinforcement-learning-problems-via-task-reduction},
    arxiv = {2103.07607},
    website = {https://sites.google.com/view/sir-compositional},
    code = {https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2FIrisLi17%2Fself-imitation-via-reduction&sa=D&sntz=1&usg=AOvVaw2WKtUQk8v-q040OqIoyMlj},
    abstract = {Our work is to train a RL agent to acquire rope-spreading and cloth-spreading skills without any human demonstrations and the method applies to real robots after domain adaptation.}

    

}
@inproceedings{wu2020learning,
  title={Learning to Manipulate Deformable Objects without Demonstrations},
  author={Wu*, Yilin and Yan*, Wilson and Kurutach, Thanard and Pinto, Lerrel and Abbeel, Pieter},
  booktitle={Robotics: Science and Systems, (RSS)},
  year={2020},
  organization={MIT Press Journals},
  arxiv = {21910.13439},
  website = {https://sites.google.com/view/alternating-pick-and-place/home},
  preview = {mvp.gif},
  abbr = {RSS},
  selected = {true},
  blog = {https://bair.berkeley.edu/blog/2020/05/05/fabrics/},
  code = {https://github.com/wilson1yan/rlpyt},
  video = {https://www.youtube.com/watch?v=7kxkJlPuLz4},
  bibtex_show = {true},
  abstract = {In this work, we introduce spatially-grounded parameterized motion primitives to improve policy generalization for robotic manipulation tasks. By grounding the primitives on a spatial location in the environment, our proposed method is able to effectively generalize across object shape and pose variations.}
}