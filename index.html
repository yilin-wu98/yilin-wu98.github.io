<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yilin Wu 吴怡琳 </title> <meta name="author" content="Yilin Wu 吴怡琳"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yilin-wu98.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <script async src="https://www.googletagmanager.com/gtag/js?id=G-HTVP7VWHHE"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-HTVP7VWHHE");</script> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yilin</span> Wu 吴怡琳 </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/yilinwu-480.webp 480w,/assets/img/yilinwu-800.webp 800w,/assets/img/yilinwu-1400.webp 1400w," sizes="(min-width: 1000px) 291.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/yilinwu.jpg?908abb4d2fbbb7e19d031ac30983326b" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="yilinwu.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p style="font-size: 16px; font-family: Roboto, sans-serif;">PhD student @ RI, SCS, CMU</p> <p style="font-size: 16px; font-family: Roboto, sans-serif;">yilinwu [at] andrew [dot] cmu [dot] edu</p> <div class="contact-icons"> <a href="mailto:%79%69%6C%69%6E%77%75@%61%6E%64%72%65%77.%63%6D%75.%65%64%75" style="font-size: 3em;" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=lyG0vMQAAAAJ" style="font-size: 3em;" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.linkedin.com/in/yilin-wu-1615b9207" style="font-size: 3em;" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/YilinWu11" style="font-size: 3em;" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note"> <p style="font-size: 16px; font-family: Roboto, sans-serif;"> Email is the best way to contact me! </p> </div> </div> </div> <div class="clearfix"> <p>I am a third-year Ph.D student at Intent Lab in CMU Robotics Institute, advised by <a href="https://www.cs.cmu.edu/~abajcsy/" rel="external nofollow noopener" target="_blank">Prof. Andrea Bajcsy</a>. Previously, I am fortunate to work with <a href="https://davheld.github.io/" rel="external nofollow noopener" target="_blank">Prof. David Held</a> on generalizable methods for long-horizon contact-rich manipulation.</p> <p>Before coming to CMU, I was a master student with a focus on assistive feeding and bimanual manipulation in Computer Science Department at Stanford University, supervised by <a href="https://dorsa.fyi/" rel="external nofollow noopener" target="_blank">Prof. Dorsa Sadigh</a>.</p> <p>In the past, I am also fortunate to work with <a href="https://jxwuyi.weebly.com/" rel="external nofollow noopener" target="_blank">Prof. Yi Wu</a> from Tsinghua University at Shanghai Qi Zhi Institute on reinforcement learning and self-imitation. In my undergrad, I also worked closely with <a href="https://people.eecs.berkeley.edu/~pabbeel/" rel="external nofollow noopener" target="_blank">Prof. Pieter Abbeel</a> and <a href="https://www.lerrelpinto.com/" rel="external nofollow noopener" target="_blank">Prof. Lerrel Pinto</a> on reinforcment learning for deformable object manpulation.</p> <p>My current research focuses on <code class="language-plaintext highlighter-rouge">open-world robot learning for manipulation</code> and <code class="language-plaintext highlighter-rouge">human-robot interaction</code>. My research centers around overcoming the fundamental <em>embodiment gap</em> that limits the application of foundation models to robotics. By grounding the high-level semantic reasoning in the continuous dynamic environment of the physical world, I want to build robots that can learn, reason and act capably in human-centered environment. My current work tries to enhance the robots’ generalization and robustness with runtime alignment and continual learning. I am also broadly interested in developing various deep learning methods like reinforcement learning and imitation learning to enhance robot’s capability for more complex manipulation tasks.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Oct 2025</th> <td> Excited to give a talk about my research on Language-Guided Runtime Steering with Robot Foundation Models at UT Austin Robin Lab! </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 2025</th> <td> Excited to give a talk about my FOREWARN paper at Robotics Team in Meta FAIR ! </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 2025</th> <td> One paper <a href="https://yilin-wu98.github.io/forewarn/">From Foresight to Forethought: VLM-in-the-loop Policy Steering via Latent Alignment</a> got accepted to RSS 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">May 2025</th> <td> Excited to start my summer internship in Nvidia Seattle Robotics Lab! </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 2024</th> <td> One paper <a href="https://sites.google.com/view/toolgen" rel="external nofollow noopener" target="_blank">Learning Generalizable Tool-use Skills through Trajectory Generation</a> got accepted at IROS 2024. </td> </tr> <tr> <th scope="row" style="width: 20%">May 2024</th> <td> <a href="">Open X-Embodiment</a> wins the Best Paper Award at ICRA 2024. </td> </tr> <tr> <th scope="row" style="width: 20%">May 2024</th> <td> Two papers <a href="https://droid-dataset.github.io/" rel="external nofollow noopener" target="_blank">DROID</a> and <a href="https://sgmp-rss2024.github.io/" rel="external nofollow noopener" target="_blank">HACMan++</a> are accepted by RSS 2024. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 2023</th> <td> Our work on <a href="https://sites.google.com/view/stabilizetoact" rel="external nofollow noopener" target="_blank">bimanual manipulation</a> inspired by human coordination is accepted to CoRL 2023 as Oral presentation. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 2023</th> <td> Starting my Ph.D study at CMU Robitic Institute. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arxiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/seal-480.webp 480w,/assets/img/publication_preview/seal-800.webp 800w,/assets/img/publication_preview/seal-1400.webp 1400w," sizes="800px" type="image/webp"> <img src="/assets/img/publication_preview/seal.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="seal.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="wu2025steering" class="col-sm-8"> <div class="title">Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification</div> <div class="author"> <em>Yilin Wu</em> , Anqi Li, Tucker Hermans, Fabio Ramos, <a href="https://www.cs.cmu.edu/~abajcsy/" rel="external nofollow noopener" target="_blank">Andrea Bajcsy<sup>*</sup></a>, and Claudia P’erez-D’Arpino<sup>*</sup> </div> <div class="periodical"> <em>In arxiv</em>, 2025 </div> <div class="periodical"> </div> <div class="award" style="font-weight: bold; color:var(--global-theme-color) ;"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2510.16281" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://yilin-wu98.github.io/steering-reasoning-vla/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA’s intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA’s own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA’s natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: this https URL</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu2025steering</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Yilin and Li, Anqi and Hermans, Tucker and Ramos, Fabio and Bajcsy, Andrea and P'erez-D'Arpino, Claudia}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arxiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RSS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/forewarn-480.webp 480w,/assets/img/publication_preview/forewarn-800.webp 800w,/assets/img/publication_preview/forewarn-1400.webp 1400w," sizes="800px" type="image/webp"> <img src="/assets/img/publication_preview/forewarn.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="forewarn.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="wu2024forewarn" class="col-sm-8"> <div class="title">From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment</div> <div class="author"> <em>Yilin Wu</em>, <a href="https://thomasrantian.github.io/" rel="external nofollow noopener" target="_blank">Ran Tian</a>, <a href="https://gokul.dev/" rel="external nofollow noopener" target="_blank">Gokul Swamy</a>, and <a href="https://www.cs.cmu.edu/~abajcsy/" rel="external nofollow noopener" target="_blank">Andrea Bajcsy</a> </div> <div class="periodical"> <em>In Robotics: Science and Systems (RSS)</em>, 2025 </div> <div class="periodical"> </div> <div class="award" style="font-weight: bold; color:var(--global-theme-color) ;">Outstanding Paper Award at ICLR 2025 World Model Workshop</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.01828" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://yilin-wu98.github.io/forewarn" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Outstanding Paper Award at ICLR 2025 World Model Workshop</p> </div> <div class="abstract hidden"> <p>While generative robot policies have demonstrated significant potential in learning complex, multimodal behaviors from demonstrations, they still exhibit diverse failures at deployment-time. Policy steering offers an elegant solution to reducing the chance of failure by using an external verifier to select from low-level actions proposed by an imperfect generative policy. Here, one might hope to use a Vision Language Model (VLM) as a verifier, leveraging its open-world reasoning capabilities. However, off-the-shelf VLMs struggle to understand the consequences of low-level robot actions as they are represented fundamentally differently than the text and images the VLM was trained on. In response, we propose FOREWARN, a novel framework to unlock the potential of VLMs as open-vocabulary verifiers for runtime policy steering. Our key idea is to decouple the VLM’s burden of predicting action outcomes (foresight) from evaluation (forethought). For foresight, we leverage a latent world model to imagine future latent states given diverse low-level action plans. For forethought, we align the VLM with these predicted latent states to reason about the consequences of actions in its native representation–natural language–and effectively filter proposed plans. We validate our framework across diverse robotic manipulation tasks, demonstrating its ability to bridge representational gaps and provide robust, generalizable policy steering.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu2024forewarn</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Yilin and Tian, Ran and Swamy, Gokul and Bajcsy, Andrea}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Robotics: Science and Systems (RSS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arxiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rapl_2024-480.webp 480w,/assets/img/publication_preview/rapl_2024-800.webp 800w,/assets/img/publication_preview/rapl_2024-1400.webp 1400w," sizes="800px" type="image/webp"> <img src="/assets/img/publication_preview/rapl_2024.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rapl_2024.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="tian2024rapl" class="col-sm-8"> <div class="title">Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment</div> <div class="author"> <a href="https://thomasrantian.github.io/" rel="external nofollow noopener" target="_blank">Ran Tian</a> , <em>Yilin Wu</em> , <a href="https://chenfengx.com/" rel="external nofollow noopener" target="_blank">Chenfeng Xu</a>, <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/" rel="external nofollow noopener" target="_blank">Masayoshi Tomizuka</a>, <a href="https://people.eecs.berkeley.edu/~malik/" rel="external nofollow noopener" target="_blank">Jitendra Malik</a>, and <a href="https://www.cs.cmu.edu/~abajcsy/" rel="external nofollow noopener" target="_blank">Andrea Bajcsy</a> </div> <div class="periodical"> <em>In submission to the International Journal of Robotics Research’s (IJRR) special issue on Foundation Models and Neural-Symbolic AI for Robotics</em>, 2024 </div> <div class="periodical"> </div> <div class="award" style="font-weight: bold; color:var(--global-theme-color) ;"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2412.04835" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://sites.google.com/berkeley.edu/rapl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains a challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user’s visual representation and then constructs a dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tian2024rapl</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tian, Ran and Wu, Yilin and Xu, Chenfeng and Tomizuka, Masayoshi and Malik, Jitendra and Bajcsy, Andrea}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{submission to  the International Journal of Robotics Research's (IJRR) special issue on Foundation Models and Neural-Symbolic AI for Robotics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/toolgen-480.webp 480w,/assets/img/publication_preview/toolgen-800.webp 800w,/assets/img/publication_preview/toolgen-1400.webp 1400w," sizes="800px" type="image/webp"> <img src="/assets/img/publication_preview/toolgen.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="toolgen.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="qitooluse2024" class="col-sm-8"> <div class="title">Learning Generalizable Tool-use Skills through Trajectory Generation</div> <div class="author"> <a href="https://carl-qi.github.io/" rel="external nofollow noopener" target="_blank">Carl Qi<sup>*</sup></a> , <em>Yilin Wu<sup>*</sup></em>, <a href="https://github.com/SilvesterYu" rel="external nofollow noopener" target="_blank">Lifan Yu</a>, <a href="https://www.ri.cmu.edu/ri-people/haoyue-liu/" rel="external nofollow noopener" target="_blank">Haoyue Liu</a>, <a href="https://www.ri.cmu.edu/ri-people/bowen-jiang-2/" rel="external nofollow noopener" target="_blank">Bowen Jiang</a>, <a href="https://xingyu-lin.github.io/" rel="external nofollow noopener" target="_blank">Xingyu Lin<sup>**</sup></a>, and <a href="https://davheld.github.io/" rel="external nofollow noopener" target="_blank">David Held<sup>**</sup></a> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024 </div> <div class="periodical"> </div> <div class="award" style="font-weight: bold; color:var(--global-theme-color) ;"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.00156" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/iheVOX8jG_8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://sites.google.com/view/toolgen" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Autonomous systems that efficiently utilize tools can assist humans in completing many common tasks such as cooking and cleaning. However, current systems fall short of matching human-level of intelligence in terms of adapting to novel tools. Prior works based on affordance often make strong assumptions about the environments and cannot scale to more complex, contact-rich tasks. In this work, we tackle this challenge and explore how agents can learn to use previously unseen tools to manipulate deformable objects. We propose to learn a generative model of the tool-use trajectories as a sequence of tool point clouds, which generalizes to different tool shapes. Given any novel tool, we first generate a tool-use trajectory and then optimize the sequence of tool poses to align with the generated trajectory. We train a single model on four different challenging deformable object manipulation tasks, using demonstration data from only one tool per task. The model generalizes to various novel tools, significantly outperforming baselines. We further test our trained policy in the real world with unseen tools, where it achieves the performance comparable to human.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">qitooluse2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Generalizable Tool-use Skills through Trajectory Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Qi, Carl and Wu, Yilin and Yu, Lifan and Liu, Haoyue and Jiang, Bowen and Lin, Xingyu and Held, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RSS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/hacman-480.webp 480w,/assets/img/publication_preview/hacman-800.webp 800w,/assets/img/publication_preview/hacman-1400.webp 1400w," sizes="800px" type="image/webp"> <img src="/assets/img/publication_preview/hacman.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="hacman.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="jiang2024hacman++" class="col-sm-8"> <div class="title">HACMan++: Spatially-Grounded Motion Primitives for Manipulation</div> <div class="author"> <a href="https://www.ri.cmu.edu/ri-people/bowen-jiang-2/" rel="external nofollow noopener" target="_blank">Bowen Jiang<sup>*</sup></a> , <em>Yilin Wu<sup>*</sup></em>, <a href="https://wenxuan-zhou.github.io/" rel="external nofollow noopener" target="_blank">Wenxuan Zhou</a>, <a href="https://cpaxton.github.io/about/" rel="external nofollow noopener" target="_blank">Chris Paxton</a>, and <a href="https://davheld.github.io/" rel="external nofollow noopener" target="_blank">David Held</a> </div> <div class="periodical"> <em>In Robotics: Science and Systems (RSS)</em>, 2024 </div> <div class="periodical"> </div> <div class="award" style="font-weight: bold; color:var(--global-theme-color) ;"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2407.08585" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://sgmp-rss2024.github.io/static/videos/all_v2.mp4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://sgmp-rss2024.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>In this work, we introduce spatially-grounded parameterized motion primitives to improve policy generalization for robotic manipulation tasks. By grounding the primitives on a spatial location in the environment, our proposed method is able to effectively generalize across object shape and pose variations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jiang2024hacman++</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{HACMan++: Spatially-Grounded Motion Primitives for Manipulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiang, Bowen and Wu, Yilin and Zhou, Wenxuan and Paxton, Chris and Held, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Robotics: Science and Systems (RSS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RSS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/droid-480.webp 480w,/assets/img/publication_preview/droid-800.webp 800w,/assets/img/publication_preview/droid-1400.webp 1400w," sizes="800px" type="image/webp"> <img src="/assets/img/publication_preview/droid.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="droid.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="khazatsky2024droid" class="col-sm-8"> <div class="title">DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset</div> <div class="author"> DROID Dataset Team </div> <div class="periodical"> <em>In Robotics: Science and Systems (RSS)</em>, 2024 </div> <div class="periodical"> </div> <div class="award" style="font-weight: bold; color:var(--global-theme-color) ;"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.12945" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://droid-dataset.github.io/videos/droid_teaser_animated.mp4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://droid-dataset.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350h of interaction data, collected across 564 scenes and 86 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance, greater robustness, and improved generalization ability.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">khazatsky2024droid</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Team, DROID Dataset}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Robotics: Science and Systems (RSS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/openx-480.webp 480w,/assets/img/publication_preview/openx-800.webp 800w,/assets/img/publication_preview/openx-1400.webp 1400w," sizes="800px" type="image/webp"> <img src="/assets/img/publication_preview/openx.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="openx.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="open_x_embodiment_rt_x_2023" class="col-sm-8"> <div class="title">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</div> <div class="author"> Open X-Embodiment Team </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, 2024 </div> <div class="periodical"> </div> <div class="award" style="font-weight: bold; color:var(--global-theme-color) ;">Best Paper Award</div> <div class="links"> <a href="http://arxiv.org/abs/2310.08864" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://robotics-transformer-x.github.io/video/teaser_compressed.mp4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://robotics-transformer-x.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Award</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">open_x_embodiment_rt_x_2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Open {X-E}mbodiment: Robotic Learning Datasets and {RT-X} Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Team, Open X-Embodiment}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CoRL</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/buds-480.webp 480w,/assets/img/publication_preview/buds-800.webp 800w,/assets/img/publication_preview/buds-1400.webp 1400w," sizes="800px" type="image/webp"> <img src="/assets/img/publication_preview/buds.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="buds.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="grannen2023stabilize" class="col-sm-8"> <div class="title">Stabilize to Act: Learning to Coordinate for Bimanual Manipulation</div> <div class="author"> <a href="https://jenngrannen.com/" rel="external nofollow noopener" target="_blank">Jennifer Grannen</a> , <em>Yilin Wu</em>, <a href="https://profiles.stanford.edu/240111" rel="external nofollow noopener" target="_blank">Brandon Vu</a>, and <a href="https://dorsa.fyi/" rel="external nofollow noopener" target="_blank">Dorsa Sadigh</a> </div> <div class="periodical"> <em>In Proceedings of the 7th Conference on Robotic Learning (CoRL)</em>, 2023 </div> <div class="periodical"> </div> <div class="award" style="font-weight: bold; color:var(--global-theme-color) ;">Oral Presentation [6.6%]</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.01087" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=nog-fOgFKdc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://sites.google.com/view/stabilizetoact" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Oral Presentation [6.6%]</p> </div> <div class="abstract hidden"> <p>We present a system for bimanual manipulation that coordinates by assigning roles to arms: a stabilizing arm holds an object stationary while an acting arm acts in this simplified environment.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">grannen2023stabilize</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Stabilize to Act: Learning to Coordinate for Bimanual Manipulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Grannen, Jennifer and Wu, Yilin and Vu, Brandon and Sadigh, Dorsa}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 7th Conference on Robotic Learning (CoRL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/bite_transfer-480.webp 480w,/assets/img/publication_preview/bite_transfer-800.webp 800w,/assets/img/publication_preview/bite_transfer-1400.webp 1400w," sizes="800px" type="image/webp"> <img src="/assets/img/publication_preview/bite_transfer.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bite_transfer.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10160467" class="col-sm-8"> <div class="title">In-Mouth Robotic Bite Transfer with Visual and Haptic Sensing</div> <div class="author"> <a href="https://lorenzos.io/" rel="external nofollow noopener" target="_blank">Lorenzo Shaikewitz<sup>*</sup></a> , <em>Yilin Wu<sup>*</sup></em>, <a href="https://suneel.belkhale.com/" rel="external nofollow noopener" target="_blank">Suneel Belkhale<sup>*</sup></a>, <a href="https://jenngrannen.com/" rel="external nofollow noopener" target="_blank">Jennifer Grannen</a>, <a href="https://priyasundaresan.github.io/" rel="external nofollow noopener" target="_blank">Priya Sundaresan</a>, and <a href="https://dorsa.fyi/" rel="external nofollow noopener" target="_blank">Dorsa Sadigh</a> </div> <div class="periodical"> <em>In 2023 IEEE International Conference on Robotics and Automation (ICRA)</em>, 2023 </div> <div class="periodical"> </div> <div class="award" style="font-weight: bold; color:var(--global-theme-color) ;"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2211.12705" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=2FeDwEpunt0&amp;ab_channel=LorenzoShaikewitz" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://hai.stanford.edu/news/building-precise-assistive-feeding-robot-can-handle-any-meal?utm_source=Stanford+HAI&amp;utm_campaign=d8369bf02f-Mailchimp_HAI_Newsletter_April+2023_3_Stanford&amp;utm_medium=email&amp;utm_term=0_aaf04f4a4b-d8369bf02f-214062174" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://sites.google.com/view/bitetransfericra2023/home" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>We build a semi autonomous robotic system to do in-mouth transfer of food safely and comfortably for disabled people. The system is composed of a force-reactive controller to safely accommodate the motions of the user throughout the transfer, a novel dexterous wrist-like end effector to reduce the discomfort and a visual sensor to identify the user mouth.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10160467</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shaikewitz, Lorenzo and Wu, Yilin and Belkhale, Suneel and Grannen, Jennifer and Sundaresan, Priya and Sadigh, Dorsa}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{In-Mouth Robotic Bite Transfer with Visual and Haptic Sensing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CoRL</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/carbs_after-480.webp 480w,/assets/img/publication_preview/carbs_after-800.webp 800w,/assets/img/publication_preview/carbs_after-1400.webp 1400w," sizes="800px" type="image/webp"> <img src="/assets/img/publication_preview/carbs_after.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="carbs_after.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="grannen2022learning" class="col-sm-8"> <div class="title">Learning Bimanual Scooping Policies for Food Acquisition</div> <div class="author"> <a href="https://jenngrannen.com/" rel="external nofollow noopener" target="_blank">Jennifer Grannen<sup>*</sup></a> , <em>Yilin Wu<sup>*</sup></em>, <a href="https://suneel.belkhale.com/" rel="external nofollow noopener" target="_blank">Suneel Belkhale</a>, and <a href="https://dorsa.fyi/" rel="external nofollow noopener" target="_blank">Dorsa Sadigh</a> </div> <div class="periodical"> <em>In Proceedings of the 6th Conference on Robot Learning (CoRL)</em>, 2022 </div> <div class="periodical"> </div> <div class="award" style="font-weight: bold; color:var(--global-theme-color) ;"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2211.14652" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://drive.google.com/file/d/1kH6bAgOLegxePHUbKUTDc5SfQpftNgsn/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://hai.stanford.edu/news/building-precise-assistive-feeding-robot-can-handle-any-meal?utm_source=Stanford+HAI&amp;utm_campaign=d8369bf02f-Mailchimp_HAI_Newsletter_April+2023_3_Stanford&amp;utm_medium=email&amp;utm_term=0_aaf04f4a4b-d8369bf02f-214062174" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://sites.google.com/view/bimanualscoop-corl22/home" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>We propose a general bimanual scooping primitive and an adaptive stabilization strategy that enables successful acquisition of a diverse set of food geometries and physical properties with close-loop visual feedback.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">grannen2022learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Bimanual Scooping Policies for Food Acquisition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Grannen, Jennifer and Wu, Yilin and Belkhale, Suneel and Sadigh, Dorsa}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 6th Conference on Robot Learning (CoRL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sir_fetch-480.webp 480w,/assets/img/publication_preview/sir_fetch-800.webp 800w,/assets/img/publication_preview/sir_fetch-1400.webp 1400w," sizes="800px" type="image/webp"> <img src="/assets/img/publication_preview/sir_fetch.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sir_fetch.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="li2021solving" class="col-sm-8"> <div class="title">Solving Compositional Reinforcement Learning Problems via Task Reduction</div> <div class="author"> <a href="https://scholar.google.com/citations?user=WvtCacIAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Yunfei Li</a> , <em>Yilin Wu</em>, <a href="http://hxu.rocks/" rel="external nofollow noopener" target="_blank">Huazhe Xu</a>, <a href="https://xiaolonw.github.io/" rel="external nofollow noopener" target="_blank">Xiaolong Wang</a>, and <a href="https://iiis.tsinghua.edu.cn/wuyi/" rel="external nofollow noopener" target="_blank">Yi Wu</a> </div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR)</em>, 2021 </div> <div class="periodical"> </div> <div class="award" style="font-weight: bold; color:var(--global-theme-color) ;"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2103.07607" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://slideslive.com/38941385/solving-compositional-reinforcement-learning-problems-via-task-reduction" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2FIrisLi17%2Fself-imitation-via-reduction&amp;sa=D&amp;sntz=1&amp;usg=AOvVaw2WKtUQk8v-q040OqIoyMlj" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/sir-compositional" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Our work is to train a RL agent to acquire rope-spreading and cloth-spreading skills without any human demonstrations and the method applies to real robots after domain adaptation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2021solving</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Solving Compositional Reinforcement Learning Problems via Task Reduction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Yunfei and Wu, Yilin and Xu, Huazhe and Wang, Xiaolong and Wu, Yi}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RSS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mvp-480.webp 480w,/assets/img/publication_preview/mvp-800.webp 800w,/assets/img/publication_preview/mvp-1400.webp 1400w," sizes="800px" type="image/webp"> <img src="/assets/img/publication_preview/mvp.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mvp.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="wu2020learning" class="col-sm-8"> <div class="title">Learning to Manipulate Deformable Objects without Demonstrations</div> <div class="author"> <em>Yilin Wu<sup>*</sup></em>, <a href="https://wilsonyan.com/" rel="external nofollow noopener" target="_blank">Wilson Yan<sup>*</sup></a>, <a href="https://thanard.github.io/" rel="external nofollow noopener" target="_blank">Thanard Kurutach</a>, <a href="https://www.lerrelpinto.com/" rel="external nofollow noopener" target="_blank">Lerrel Pinto</a>, and <a href="https://people.eecs.berkeley.edu/~pabbeel/" rel="external nofollow noopener" target="_blank">Pieter Abbeel</a> </div> <div class="periodical"> <em>In Robotics: Science and Systems, (RSS)</em>, 2020 </div> <div class="periodical"> </div> <div class="award" style="font-weight: bold; color:var(--global-theme-color) ;"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/21910.13439" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=7kxkJlPuLz4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://bair.berkeley.edu/blog/2020/05/05/fabrics/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/wilson1yan/rlpyt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/alternating-pick-and-place/home" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>In this work, we introduce spatially-grounded parameterized motion primitives to improve policy generalization for robotic manipulation tasks. By grounding the primitives on a spatial location in the environment, our proposed method is able to effectively generalize across object shape and pose variations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu2020learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Manipulate Deformable Objects without Demonstrations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Yilin and Yan, Wilson and Kurutach, Thanard and Pinto, Lerrel and Abbeel, Pieter}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Robotics: Science and Systems, (RSS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{MIT Press Journals}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yilin Wu 吴怡琳. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>